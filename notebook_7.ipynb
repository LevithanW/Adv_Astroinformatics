{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Advanced Astroinformatics (Semester 1 2025)**\n",
    "# Supervised Classification, Data Processing Pipelines\n",
    "\n",
    "**Advanced Astroinformatics Student Project**\n",
    "\n",
    "*N. Hernitschek*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Contents\n",
    "* [Recap, Questions](#first-bullet)\n",
    "* [Data Processing Pipelines](#second-bullet)\n",
    "* [Summary](#fifth-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap, Questions <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "Time for questions!\n",
    "\n",
    "Your **tasks until this week** were:\n",
    "\n",
    "Based on what you have seen here: Use a multiclass supervised machine learning algorithm on the three TESS feature data sets, including making diagnostic plots and the classification scores.\n",
    "\n",
    "Hint: Use the `scikit-learn` documentation (a good starting point: https://scikit-learn.org/stable/modules/multiclass.html). You can also generally search for code examples and reuse parts of the code. Reusing code is a great way to learn. As always: When reusing code, never use this without understanding what the code does!\n",
    "\n",
    "What are your **results** so far? Can you show plots?\n",
    " \n",
    "\n",
    "If this works: \n",
    "\n",
    "Use the k-means algorithm on the three TESS feature data sets, including making diagnostic plots.\n",
    "\n",
    "Try to interpret your results.\n",
    "How do your results differ from the a) _TESS_lightcurves_outliercleaned, b) _TESS_lightcurves_median_after_detrended, c) _TESS_lightcurves_raw?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing Pipelines <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "We already have seen *data processing pipelines* to some extent:\n",
    "\n",
    "* TESS light-curve data were first detrended and outlier-cleaned: _TESS_lightcurves_raw $\\rightarrow$ _TESS_lightcurves_median_after_detrended $\\rightarrow$  _TESS_lightcurves_outliercleaned\n",
    "* features were calculated from (outlier-cleaned) TESS light-curve data\n",
    "* features were used for classification\n",
    "\n",
    "**Question:** \n",
    "Can you think of ways to improve the classification process?\n",
    "What does \"improve\" exactly mean? What we want to achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large existing and upcoming surveys, such as LSST, rely and will rely heavily on data processing pipelines, e.g. for LSST:\n",
    "    \n",
    "    \n",
    "https://www.lsst.org/about/dm/pipelines\n",
    "\n",
    "https://antares.noirlab.edu/pipeline\n",
    "    \n",
    "Such surveys have the goal to:\n",
    "\n",
    "* find \"unknowns\", \"unknown unknowns\"\n",
    "* find more examples of the same types to build catalogs.\n",
    "    \n",
    "**Question:**\n",
    "How is this related to the science you are doing currently, and/or are planning to do?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc47b2710824c32a75e3037ccc7a397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=22, continuous_update=False, description='Class Label:', max=22), IntRanâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import pandas as pd\n",
    "import os\n",
    "from astropy.timeseries import LombScargle\n",
    "import numpy as np\n",
    "import corner\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import feets\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, adjusted_rand_score, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "path = \"/Users/guillermow/Documents/PhD/1st Semester/Advanced Astroinformatics/_data/\"\n",
    "lc_median= path + \"_TESS_lightcurves_median_after_detrended/\"\n",
    "lc_raw=path + '_TESS_lightcurves_raw/'\n",
    "lc_cleaned=path + '_TESS_lightcurves_outliercleaned/'\n",
    "info_tess_data=path + 'info_tess_data.txt'\n",
    "\n",
    "paths_raw = sorted([str(p) for p in Path(lc_raw).iterdir()])\n",
    "paths_raw = paths_raw[1:] \n",
    "\n",
    "paths_median = sorted([str(p) for p in Path(lc_median).iterdir()])\n",
    "paths_median = paths_median[1:]\n",
    "\n",
    "paths_cleaned = sorted([str(p) for p in Path(lc_cleaned).iterdir()])\n",
    "paths_cleaned = paths_cleaned[1:] \n",
    "\n",
    "suffix_cleaned = np.array([Path(a).name for a in paths_cleaned])\n",
    "suffix_raw=suffix_cleaned\n",
    "subtext_median= '_lc_median_after_cbv_detrended_'\n",
    "suffix_median=np.array([subtext_median + a for a in suffix_raw])\n",
    "\n",
    "X_clean_1ft = []\n",
    "X_raw_1ft = []\n",
    "X_median_1ft = []\n",
    "\n",
    "X_clean_2ft = []\n",
    "X_raw_2ft = []\n",
    "X_median_2ft = []\n",
    "\n",
    "y = []\n",
    "\n",
    "a=4\n",
    "b=2\n",
    "\n",
    "for suffix in suffix_cleaned:\n",
    "    folder_path = f'/Users/guillermow/Documents/PhD/1st Semester/Advanced Astroinformatics/_data/features/{suffix}'\n",
    "    df_clean = pd.read_csv(f'{folder_path}/clean_feats.csv', index_col=0)\n",
    "    df_raw = pd.read_csv(f'{folder_path}/raw_feats.csv', index_col=0)\n",
    "    df_median = pd.read_csv(f'{folder_path}/median_feats.csv', index_col=0)\n",
    "    \n",
    "    cols = df_clean.columns[:-1] \n",
    "    X_clean_1ft.append(df_clean[cols[a]].values)\n",
    "    X_raw_1ft.append(df_raw[cols[a]].values)\n",
    "    X_median_1ft.append(df_median[cols[a]].values)\n",
    "    \n",
    "    X_clean_2ft.append(df_clean[cols[b]].values)\n",
    "    X_raw_2ft.append(df_raw[cols[b]].values)\n",
    "    X_median_2ft.append(df_median[cols[b]].values)\n",
    "    \n",
    "    y.append([suffix for _ in range(len(df_clean[cols[0]].values))])\n",
    "    \n",
    "feats = df_clean.columns\n",
    "X_1ft = np.concatenate(X_raw_1ft)\n",
    "X_2ft = np.concatenate(X_raw_2ft)\n",
    "X = np.vstack([X_1ft, X_2ft]).T\n",
    "\n",
    "y_string = np.concatenate(y)\n",
    "y = le.fit_transform(y_string) #Turning the labels into numerical values\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, title):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    norm_conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1, keepdims=True)\n",
    "    if 'suffix_cleaned' in globals() and len(suffix_cleaned) == conf_mat.shape[0]:\n",
    "        class_labels = suffix_cleaned\n",
    "    else:\n",
    "        class_labels = np.arange(conf_mat.shape[0])\n",
    "    sns.heatmap(norm_conf_mat, \n",
    "                xticklabels=class_labels, \n",
    "                yticklabels=class_labels,\n",
    "                cmap=\"Blues\", annot=True, fmt=\".2f\", cbar=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X, y, title, num_label, ab):\n",
    "    \n",
    "    X_clean_1ft = []\n",
    "    X_raw_1ft = []\n",
    "    X_median_1ft = []\n",
    "\n",
    "    X_clean_2ft = []\n",
    "    X_raw_2ft = []\n",
    "    X_median_2ft = []\n",
    "\n",
    "    y = []\n",
    "\n",
    "    a, b = ab\n",
    "\n",
    "    for suffix in suffix_cleaned:\n",
    "        folder_path = f'/Users/guillermow/Documents/PhD/1st Semester/Advanced Astroinformatics/_data/features/{suffix}'\n",
    "        df_clean = pd.read_csv(f'{folder_path}/clean_feats.csv', index_col=0)\n",
    "        df_raw = pd.read_csv(f'{folder_path}/raw_feats.csv', index_col=0)\n",
    "        df_median = pd.read_csv(f'{folder_path}/median_feats.csv', index_col=0)\n",
    "        \n",
    "        cols = df_clean.columns[:-1] \n",
    "        X_clean_1ft.append(df_clean[cols[a]].values)\n",
    "        X_raw_1ft.append(df_raw[cols[a]].values)\n",
    "        X_median_1ft.append(df_median[cols[a]].values)\n",
    "        \n",
    "        X_clean_2ft.append(df_clean[cols[b]].values)\n",
    "        X_raw_2ft.append(df_raw[cols[b]].values)\n",
    "        X_median_2ft.append(df_median[cols[b]].values)\n",
    "        \n",
    "        y.append([suffix for _ in range(len(df_clean[cols[0]].values))])\n",
    "        \n",
    "    feats = df_clean.columns\n",
    "    X_1ft = np.concatenate(X_raw_1ft)\n",
    "    X_2ft = np.concatenate(X_raw_2ft)\n",
    "    X = np.vstack([X_1ft, X_2ft]).T\n",
    "\n",
    "    y_string = np.concatenate(y)\n",
    "    y = le.fit_transform(y_string) #Turning the labels into numerical values\n",
    "    \n",
    "    cmap = plt.get_cmap('gnuplot')\n",
    "    folds = 10\n",
    "    k_fold = KFold(folds, shuffle=True, random_state=1)\n",
    "\n",
    "    predicted_targets = np.array([])\n",
    "    actual_targets = np.array([])\n",
    "\n",
    "    input_folds = np.empty(folds, dtype=object)\n",
    "    result_folds = np.empty(folds, dtype=object)\n",
    "\n",
    "    fold = 0\n",
    "\n",
    "    for train_ix, test_ix in k_fold.split(X):\n",
    "        train_x, train_y = X[train_ix], y[train_ix]\n",
    "        test_x, test_y = X[test_ix], y[test_ix]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=10, criterion='gini', random_state=0, bootstrap=True)\n",
    "        classifier.fit(train_x, train_y)\n",
    "        predicted_labels = classifier.predict(test_x)\n",
    "\n",
    "        predicted_targets = np.append(predicted_targets, predicted_labels)\n",
    "        actual_targets = np.append(actual_targets, test_y)\n",
    "\n",
    "        input_folds[fold] = test_y\n",
    "        result_folds[fold] = predicted_labels\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    if num_label < 22:\n",
    "        predicted_targets = predicted_targets.astype(int)\n",
    "        mask1 = (predicted_targets == num_label)\n",
    "        string_label = le.inverse_transform([num_label])[0] \n",
    "\n",
    "        idx = num_label\n",
    "        unique_numerical_labels_test = np.unique(predicted_targets)\n",
    "\n",
    "        ax[0].scatter(\n",
    "            X[mask1, 0], X[mask1, 1],\n",
    "            c=[cmap(idx / len(unique_numerical_labels_test))],\n",
    "            alpha=0.6,\n",
    "            label=string_label,\n",
    "            edgecolors='k',\n",
    "            s=10,\n",
    "        )\n",
    "        ax[0].set_title(\"Test Predictions\")\n",
    "        ax[0].legend()\n",
    "        ax[0].set_xlim(X[:, 0].min() - 0.15, X[:, 0].max() + 0.15)\n",
    "        ax[0].set_ylim(X[:, 1].min() - 0.15, X[:, 1].max() + 0.15)\n",
    "        ax[0].set_ylabel(feats[b])\n",
    "\n",
    "        mask2 = (y == num_label)\n",
    "        unique_numerical_labels_train = np.unique(y)\n",
    "\n",
    "        ax[1].scatter(\n",
    "            X[mask2, 0], X[mask2, 1],\n",
    "            c=[cmap(idx / len(unique_numerical_labels_train))],\n",
    "            label=string_label,\n",
    "            alpha=0.6,\n",
    "            edgecolors='k',\n",
    "            s=10,\n",
    "        )\n",
    "        ax[1].set_title(\"Training Samples\")\n",
    "        ax[1].legend()\n",
    "        ax[1].set_xlim(X[:, 0].min() - 0.15, X[:, 0].max() + 0.15)\n",
    "        ax[1].set_ylim(X[:, 1].min() - 0.15, X[:, 1].max() + 0.15)\n",
    "        ax[1].set_xlabel(feats[a])\n",
    "\n",
    "        plt.suptitle(f\"Decision Boundary - {title}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        unique_numerical_labels_test = np.unique(predicted_targets)\n",
    "        unique_numerical_labels_train = np.unique(y)\n",
    "\n",
    "        for idx, num in enumerate(unique_numerical_labels_train):\n",
    "            mask1 = (predicted_targets == num)\n",
    "            string_label = le.inverse_transform([int(num)])[0]\n",
    "\n",
    "            ax[0].scatter(\n",
    "                X[mask1, 0], X[mask1, 1],\n",
    "                c=[cmap(idx / len(unique_numerical_labels_test))],\n",
    "                label=string_label,\n",
    "                alpha=0.5,\n",
    "                edgecolors='k',\n",
    "                s=10,\n",
    "            )\n",
    "            ax[0].set_title(\"Test Predictions\")\n",
    "            ax[0].set_ylabel(feats[b])\n",
    "            #ax[0].legend()\n",
    "            ax[0].set_xlim(X[:, 0].min() - 0.15, X[:, 0].max() + 0.15)\n",
    "            ax[0].set_ylim(X[:, 1].min() - 0.15, X[:, 1].max() + 0.15)\n",
    "            \n",
    "            \n",
    "        for idx, num in enumerate(unique_numerical_labels_test):\n",
    "            mask2 = (y == num)\n",
    "            string_label = le.inverse_transform([int(num)])[0] \n",
    "\n",
    "            ax[1].scatter(\n",
    "                X[mask2, 0], X[mask2, 1],\n",
    "                c=[cmap(idx / len(unique_numerical_labels_train))],\n",
    "                alpha=0.5,\n",
    "                label=string_label,\n",
    "                edgecolors='k',\n",
    "                s=10,\n",
    "            )\n",
    "            ax[1].set_title(\"Training Samples\")\n",
    "            ax[1].set_xlabel(feats[a])\n",
    "            ax[1].legend(bbox_to_anchor=(1, 1.125))\n",
    "            ax[1].set_xlim(X[:, 0].min() - 0.15, X[:, 0].max() + 0.15)\n",
    "            ax[1].set_ylim(X[:, 1].min() - 0.15, X[:, 1].max() + 0.15)\n",
    "            \n",
    "        plt.suptitle(f\"Feature Comparison - {title}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        cm = confusion_matrix(actual_targets.astype(int), predicted_targets.astype(int))\n",
    "        \n",
    "        plot_confusion_matrix(cm, title=f\"Confusion Matrix - {title}\")\n",
    "\n",
    "results = {}\n",
    "versions = ['Raw', 'Clean', 'Median']\n",
    "\n",
    "X_clean = np.vstack([np.concatenate(X_clean_1ft), np.concatenate(X_clean_2ft)]).T\n",
    "X_raw = np.vstack([np.concatenate(X_raw_1ft), np.concatenate(X_raw_2ft)]).T\n",
    "X_median = np.vstack([np.concatenate(X_median_1ft), np.concatenate(X_median_2ft)]).T\n",
    "\n",
    "data_versions = [X_raw, X_clean, X_median]\n",
    "x_data = data_versions[0]\n",
    "\n",
    "num_label = widgets.IntSlider(\n",
    "    value=22,\n",
    "    min=0,\n",
    "    max=len(np.unique(y)),\n",
    "    step=1,\n",
    "    description='Class Label:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "ab_slider = widgets.IntRangeSlider(\n",
    "    value=[4, 5],\n",
    "    min=0,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Types of Feature:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "\n",
    "#plot_decision_boundary(x_data, y, f\"{versions[0]}\", num_label=0)\n",
    "interact(plot_decision_boundary, X=fixed(x_data), y=fixed(y), title=fixed(versions[1]), num_label=num_label, ab=ab_slider);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "At this point, you should have:\n",
    "\n",
    "\n",
    "* seen how `scikit-learn` works in general\n",
    "* seen some complete examples of machine learning for both unsupervised and supervised classification in the case of binary and multiclass classification\n",
    "* seen ways on how to verify machine learning results for both unsupervised and supervised classification in the case of binary and multiclass classification.\n",
    "* seen how machine learning, and data processing pipelines in general, fit into the larger picture in processing astronomical data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
